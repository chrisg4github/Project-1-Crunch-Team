{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##Dependencies\n",
    "from matplotlib.figure import Figure\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import random as rd\n",
    "np.random.seed(sum(map(ord, \"aesthetics\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Read file and then sort \n",
    "csv_file = \"companies.csv\"\n",
    "companies_df = pd.read_csv(csv_file, encoding=\"iso-8859-1\",\n",
    "                           parse_dates=[\"Founded Date\",\"Closed Date\",\n",
    "                                        \"Last Funding Date\"])\n",
    "companies_df = companies_df.sort_values(\"Company Name\").reset_index(drop=True)\n",
    "companies_df.head(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "companies_df = companies_df[[\"Company Name\", \"Founded Date\", \"Closed Date\", \n",
    "                             \"Number of Funding Rounds\", \"Last Funding Date\", \n",
    "                             \"Last Funding Amount\", \"Total Funding Amount\", \"Status\"]]\n",
    "companies_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "companies_pf = companies_df.rename(columns={\n",
    "    \"Company Name\" : \"Company\",\n",
    "    \"Number of Funding Rounds\" : \"Funding Rounds\"   \n",
    "})\n",
    "\n",
    "companies_pf.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Read file and then sort \n",
    "funding = \"Funding_Rounds.csv\"\n",
    "funding_df = pd.read_csv(funding, encoding=\"iso-8859-1\", \n",
    "                         parse_dates=[\"Announced On Date\"])\n",
    "funding_df.head(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "funding_df = funding_df[[\"Company Name\", \"Funding Type\", \"Money Raised\", \n",
    "                         \"Announced On Date\"]]\n",
    "funding_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "funding_pf = funding_df.rename(columns={\n",
    "    \"Company Name\" : \"Company\"\n",
    "})\n",
    "\n",
    "funding_pf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge companies_pf and funding_pf on Company\n",
    "merged_data = pd.merge(companies_pf, funding_pf,how=\"inner\",on=\"Company\")\n",
    "merged_data.head(5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Save Data to csv without index, with header\n",
    "merged_data.to_csv(\"merged_company_funding.csv\", index=False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Read file and then sort \n",
    "org = \"organizations.csv\"\n",
    "org_df = pd.read_csv(org, encoding=\"iso-8859-1\")\n",
    "org_df.head(5)                              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename columns for readability\n",
    "new_file = org_df.rename(columns={\n",
    "    \"crunchbase_uuid\": \"UUID\",\n",
    "    \"Company Name\" : \"Company\",\n",
    "    \"homepage_url\" : \"Homepage\"   \n",
    "})\n",
    "\n",
    "new_file.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_file = new_file[[\"UUID\", \"Company\", \"Homepage\"]]\n",
    "new_file.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Save Data to csv without index, with header\n",
    "new_file.to_csv(\"new_org.csv\", index=False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = pd.merge(merged_data, new_file,how=\"inner\",on=\"Company\")\n",
    "merged_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "merged_df[\"Money Raised\"] = merged_df[\"Money Raised\"].str.strip() \\\n",
    "                                                   .replace('[\\$,]','', regex=True ).astype(np.int64)\n",
    "    \n",
    "merged_df[\"Last Funding Amount\"] = merged_df[\"Last Funding Amount\"].str.strip() \\\n",
    "                                                   .replace('[\\$,]','', regex=True ).astype(np.int64)\n",
    "merged_df[\"Total Funding Amount\"] = merged_df[\"Total Funding Amount\"].str.strip() \\\n",
    "                                                   .replace('[\\$,]','', regex=True ).astype(np.int64)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Save Data to csv without index, with header\n",
    "merged_df.to_csv(\"all_files_merged_df.csv\", index=False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# works\n",
    "for i, el in enumerate(merged_df.iterrows()):\n",
    "    val = el[1]['UUID'].replace(\"-\", \"\")\n",
    "    merged_df.set_value(i, \"UUID\", val)\n",
    "merged_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "status = merged_df.groupby('Funding Type')[\"Status\"].value_counts()\n",
    "\n",
    "#g1 = df1.groupby([\"Name\", \"City\"], as_index=False).count()\n",
    "\n",
    "status.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.DataFrame(status).rename(columns={\n",
    "    'Status':'Count'})\n",
    "\n",
    "df1.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "series_d = merged_df[merged_df['Funding Type'] == \"Series D\"]\n",
    "series_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "funding_df = merged_df[merged_df['Funding Type'] == \"Seed\"]\n",
    "funding_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = merged_df[merged_df['Status'] == \"Was Acquired\"].count\n",
    "seed_df = merged_df[merged_df['Funding Type'] == \"Seed\"]\n",
    "\n",
    "seed_df = seed_df[seed_df['Status'] == \"Was Acquired\"]\n",
    "\n",
    "seed_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # Build a scatter plot for each data type\n",
    "# plt.scatter(seed_df[\"Funding Type\"], \n",
    "#             seed_df[\"Status\"],\n",
    "#             s=100, edgecolor=\"black\", linewidths=1, c=\"red\", marker=\"o\", \n",
    "#             alpha=0.8)\n",
    "\n",
    "# # Incorporate the other graph properties\n",
    "# plt.title(\"Funding Type Status\")\n",
    "# plt.ylabel(\"Funding Type\")\n",
    "# plt.xlabel(\"Status\")\n",
    "# plt.grid(True)\n",
    "\n",
    "# # Save the figure\n",
    "# plt.savefig(\"funding_type_scatter_chart.png\")\n",
    "\n",
    "# # Show plot\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# #seed = merged_df[merged_df['Status'] == \"Operating\"]\n",
    "# seed = merged_df[merged_df['Status'] == \"Was Acquired\"]\n",
    "# seed = merged_df[merged_df['Status'] == \"Closed\"]\n",
    "# seed = merged_df[merged_df['Status'] == \"IPO\"]\n",
    "\n",
    "# # # x-axis\n",
    "# seed_operating = seed.groupby([\"Status\"]).count()[\"Closed\"]\n",
    "# # #y-axis\n",
    "# seed_acquired = seed.groupby([\"Status\"]).count()[\"Was Acquired\"]\n",
    "\n",
    "# # # x-axis\n",
    "# seed_closedt = seed.groupby([\"Status\"]).count()[\"Closed\"]\n",
    "# # #y-axis\n",
    "# seed_ips = seed.groupby([\"Status\"]).count()[\"IPO\"]\n",
    "\n",
    "# # # # x-axis\n",
    "# # rural_ride_count = rural_cities.groupby([\"city\"]).count()[\"fare\"]\n",
    "# # # #y-axis\n",
    "# # rural_driver_count = rural_cities.groupby([\"city\"]).mean()[\"driver_count\"]\n",
    "\n",
    "# # # # x-axis\n",
    "# # suburban_ride_count = suburban_cities.groupby([\"city\"]).count()[\"fare\"]\n",
    "# # # #y-axis\n",
    "# # suburban_driver_count = suburban_cities.groupby([\"city\"]).mean()[\"driver_count\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "funding_sum = merged_df.groupby('Funding Type')[\"Money Raised\"].sum()\n",
    "funding_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['font.size'] = 12.0\n",
    "fig = plt.figure(figsize=[8, 8])\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "types_area = [\"Seed\", \"Series A\", \"Series B\", \"Series C\",\"Series D\" ]\n",
    "numbers = funding_sum\n",
    "colors = [\"gold\", \"lightcoral\", \"cyan\", \"lightgreen\", \"lightskyblue\"]\n",
    "explode = (0, 0.06, 0, 0, 0)\n",
    "\n",
    "x_axis = np.arange(0, len(types_area))\n",
    "\n",
    "ax.set_title(\"Funding Type Total Funding Percentages for Startups\", \n",
    "             weight='bold').set_fontsize('18')\n",
    "ax.pie(numbers, explode=explode, labels=types_area, colors=colors,\n",
    "       autopct=\"%1.2f%%\", textprops={'weight': 'bold', 'fontsize':'14'},\n",
    "       shadow=True, startangle=30)\n",
    "ax.axis(\"equal\")\n",
    "\n",
    "# Save the figure\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"funding_type_sum_pie_chart.png\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sum of counts is 10278\n",
    "funding_pct = merged_df.groupby('Funding Type')[\"Money Raised\"].count()\n",
    "funding_pct\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['font.size'] = 12.0\n",
    "\n",
    "fig = plt.figure(figsize=[8, 8])\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "types_area = [\"Seed\", \"Series A\", \"Series B\", \"Series C\",\"Series D\" ]\n",
    "numbers = funding_pct\n",
    "colors = [\"gold\", \"lightcoral\", \"cyan\", \"lightgreen\", \"lightskyblue\"]\n",
    "explode = (0.06, 0, 0, 0, 0)\n",
    "\n",
    "x_axis = np.arange(0, len(types_area))\n",
    "\n",
    "ax.set_title(\"Funding Type Percentages for Startups\", weight='bold').set_fontsize('18')\n",
    "\n",
    "ax.pie(numbers, explode=explode, \n",
    "       labels=types_area, colors=colors,\n",
    "       autopct=\"%1.2f%%\", textprops={'weight': 'bold', 'fontsize':'14'}, \n",
    "       shadow=True, startangle=30, pctdistance=.85, labeldistance=1.1)\n",
    "\n",
    "ax.axis(\"equal\")\n",
    "\n",
    "# Save the figure\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"funding_type_count_pie_chart.png\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# y axis for funding_average_per_type_bar_chart \n",
    "fund_avgs = np.array(merged_df.groupby('Funding Type')[\"Money Raised\"].median())\n",
    "#fund_avgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# x axis for funding_average_per_type_bar_chart \n",
    "funding_types = merged_df[\"Funding Type\"].unique()\n",
    "#funding_types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# funding_average_per_type_bar_chart \n",
    "# Dependencies\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "x_axis = np.arange(1, len(funding_types)+ 1)\n",
    "xtick_locations = [x for x in x_axis]\n",
    "\n",
    "ax.set_title(\"Funding Type Average Investment\", weight='bold').set_fontsize('18')\n",
    "ax.set_xlabel(\"Funding Types\", weight='bold').set_fontsize('14')\n",
    "ax.set_ylabel(\"Average Investments(10MM)\", weight='bold').set_fontsize('14')\n",
    "\n",
    "ax.set_xlim(0, len(funding_types)+ 1)\n",
    "\n",
    "plt.ticklabel_format(style = 'plain')\n",
    "\n",
    "ax.bar(x_axis, fund_avgs, facecolor=\"red\", width=.4)\n",
    "ax.set_xticks(xtick_locations)\n",
    "\n",
    "ax.set_xticklabels(funding_types, rotation=35, weight='bold')\n",
    "ax.set_yticklabels([0,10,20,30,40,50,60,70],\n",
    "                   rotation=360, weight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"funding_type_avg_investment.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## File without url for screen scraping\n",
    "## Read file and then sort \n",
    "csv_file = \"top_orgs.csv\"\n",
    "top_orgs = pd.read_csv(csv_file, encoding=\"iso-8859-1\", parse_dates=[\"Announced On Date\"])\n",
    "top_orgs = top_orgs.sort_values(\"Funding Type\").reset_index(drop=True)\n",
    "top_orgs.head(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "top_orgs[\"Money Raised\"] = top_orgs[\"Money Raised\"].replace('[\\$,]','', regex=True ).astype(np.int64)\n",
    "top_orgs[\"Homepage\"] = top_orgs[\"Homepage\"].replace(np.nan, '', regex=True)\n",
    "\n",
    "#top_orgs[\"Homepage\"] = \"\"\n",
    "top_orgs[\"Total Visits\"] = \"\"\n",
    "top_orgs[\"Avg Visit Duration\"] = \"\"\n",
    "top_orgs[\"Pages Per Visit\"] = \"\"\n",
    "top_orgs[\"Bounce Rate\"] = \"\"\n",
    "print(len(top_orgs.index))\n",
    "top_orgs.head(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # The original top_orgs.xslx had the wrong url so this is a \n",
    "# # manual process to get the Homepage for Screen Scraping.\n",
    "# # The automated process can be built on this process.\n",
    "# # \n",
    "# # 1) Remove duplicates from copy of merged_df\n",
    "# # 2) Copy the output into top_orgs_search_results.txt\n",
    "# # 3) Manually copy the homepage url into top_orgs.xlsx and\n",
    "# #    create top_orgs.csv.\n",
    "# # 4) Searched organizations.xlsx manually and found Ripcord, \n",
    "# #    IndoorAtlas, and Truss.\n",
    "# # 5) Found 46 of 50 urls.\n",
    "\n",
    "# Copy merged_df to a temp data frame for processing\n",
    "tmp_merged_df = merged_df\n",
    "\n",
    "# Remove duplicates method 1:\n",
    "# tmp_merged_df = tmp_merged_df.groupby([\"Company\",\"Homepage\"]).max()\n",
    "# tmp_merged_df.reset_index(inplace=True)\n",
    "# or\n",
    "# Remove duplicates method 2:\n",
    "tmp_merged_df = tmp_merged_df.drop_duplicates(subset=[\"Company\",\"Homepage\"],\n",
    "                                              keep=\"last\").reset_index(drop=True)\n",
    "\n",
    "count = 0\n",
    "\n",
    "## Iterate over the top_orgs rows and lookup the homepage via the Company\n",
    "## This doesn't give what I want so I manually copied the url into top_orgs.xlsx\n",
    "## and created top_orgs.csv.\n",
    "for index, row in top_orgs.iterrows():\n",
    "    try:\n",
    "        # Use Company to get Homepage\n",
    "        print(\"This is the result for: \", row[\"Company\"])\n",
    "        #print(tmp_merged_df.loc[tmp_merged_df[\"Company\"] == row[\"Company\"],[\"Homepage\"]].values)\n",
    "        print(tmp_merged_df.loc[tmp_merged_df[\"Company\"] == row[\"Company\"],[\"Homepage\"]])\n",
    "        count += 1\n",
    "        ## Set the cell info for Homepage (Doesn't work!)\n",
    "        #print(df.loc[df['D'] == 14]['A'].values)\n",
    "        #url = tmp_merged_df.loc[tmp_merged_df[\"Company\"] == row[\"Company\"],[\"Homepage\"]]\n",
    "        #top_orgs.set_value(index, \"Homepage\", url)\n",
    "    except:\n",
    "        print(\"Error for: \", row[\"Company\"])  \n",
    "\n",
    "print(\"top_orgs rows processed: \", count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Web scraping program with 20 second sleep time.\n",
    "## Consider using a smaller subset of data for scraping.\n",
    "\n",
    "##  Test URL https://www.similarweb.com/website/shotput.com#overview \n",
    "\n",
    "## Splinter documenation:\n",
    "## https://github.com/douglasmiranda/splinter-examples/blob/master/another_examples/screenshot.py\n",
    "## http://splinter.readthedocs.io/en/latest/tutorial.html\n",
    "\n",
    "## BeautifulSoup Documentation:\n",
    "## https://www.crummy.com/software/BeautifulSoup/bs4/doc\n",
    "\n",
    "\n",
    "from splinter import Browser\n",
    "import csv\n",
    "from bs4 import BeautifulSoup \n",
    "import time\n",
    "\n",
    "# browser = Browser('chrome')\n",
    "# browser = Browser('firefox')\n",
    "\n",
    "with Browser() as browser:\n",
    "    # Visit URL\n",
    "    url1 = \"https://www.similarweb.com/website/\" \n",
    "    url_suffix = \"#overview\"\n",
    "    \n",
    "    # loop thru the urls top_orgs data frame \n",
    "    counter = 0\n",
    "    for index, row in top_orgs.iterrows():\n",
    "        url = \"\"\n",
    "        #print(type(row[\"Homepage\"]))\n",
    "        url2 = row[\"Homepage\"].split('/')\n",
    "        if len(url2) > 2:\n",
    "            counter += 1\n",
    "            url = url1 + url2[2] + url_suffix\n",
    "            print(\"url counter: \", counter,end=\" - \")\n",
    "            print(url)\n",
    "        \n",
    "            try:\n",
    "                browser.visit(url)\n",
    "                time.sleep(20)\n",
    "                html = browser.html\n",
    "                soup = BeautifulSoup(html, \"html.parser\")\n",
    "            \n",
    "                #print(soup.prettify())\n",
    "   \n",
    "                count = 0\n",
    "                # Grab the 4 activity stats from the html \n",
    "                for line in soup.findAll('span',class_=\"engagementInfo-valueNumber js-countValue\"):\n",
    "                    print(line.get_text())\n",
    "                    count += 1\n",
    "                    # Write to a data frame\n",
    "                    if count == 1:\n",
    "                        top_orgs.set_value(index, \"Total Visits\", line.getText())\n",
    "                    elif count == 2:\n",
    "                        top_orgs.set_value(index, \"Avg Visit Duration\", line.getText())\n",
    "                    elif count == 3:\n",
    "                        top_orgs.set_value(index, \"Pages Per Visit\", line.getText())\n",
    "                    else:\n",
    "                        top_orgs.set_value(index, \"Bounce Rate\", line.getText())\n",
    "            except:\n",
    "                print(\"Error with url: \", url)  \n",
    "top_orgs\n",
    "print(\"urls processed: \", counter)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Save Data to csv without index, with header\n",
    "top_orgs.to_csv(\"screen_scrape_data.csv\", index=False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# top_orgs[\"Avg Visit Duration\"] = pd.to_datetime(top_orgs[\"Avg Visit Duration\"], format=\"%H:%M:%S\").dt.time\n",
    "# import datetime as dt  dt=datetime.strptime(start,\"%H:%M:%S\").time()\n",
    "# import re  line = re.sub('[!@#$]', '', line)\n",
    "top_orgs\n",
    "\n",
    "# top_orgs[\"Total Visits\"] = top_orgs[\"Total Visits\"].replace('[<>K]','', regex=True ) \\\n",
    "# .astype(np.int32) * 1000\n",
    "\n",
    "# #top_orgs[\"Avg Visit Duration\"] = pd.to_datetime(top_orgs[\"Avg Visit Duration\"],format=\"%H:%M:%S\")\n",
    "\n",
    "# top_orgs[\"Bounce Rate\"] = top_orgs[\"Bounce Rate\"].replace('[%]','', regex=True ).astype(np.float64)\n",
    "\n",
    "# top_orgs[\"Pages Per Visit\"] = top_orgs[\"Pages Per Visit\"].astype(np.float64)\n",
    "# top_orgs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
