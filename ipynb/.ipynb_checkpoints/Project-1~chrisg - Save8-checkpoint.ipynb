{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##Dependencies\n",
    "from matplotlib.figure import Figure\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import random as rd\n",
    "np.random.seed(sum(map(ord, \"aesthetics\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "File b'companies.csv' does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-bc84c0e142fe>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m companies_df = pd.read_csv(csv_file, encoding=\"iso-8859-1\",\n\u001b[0;32m      4\u001b[0m                            parse_dates=[\"Founded Date\",\"Closed Date\",\n\u001b[1;32m----> 5\u001b[1;33m                                         \"Last Funding Date\"])\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[0mcompanies_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompanies_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msort_values\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Company Name\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mcompanies_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Chrisg\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, skip_footer, doublequote, delim_whitespace, as_recarray, compact_ints, use_unsigned, low_memory, buffer_lines, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    653\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[0;32m    654\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 655\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    656\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    657\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Chrisg\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    403\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    404\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 405\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    406\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    407\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Chrisg\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    760\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'has_index_names'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'has_index_names'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    761\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 762\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    763\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    764\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Chrisg\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m    964\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'c'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    965\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'c'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 966\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    967\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    968\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'python'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Chrisg\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   1580\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'allow_leading_cols'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex_col\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1581\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1582\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1583\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1584\u001b[0m         \u001b[1;31m# XXX\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__ (pandas\\_libs\\parsers.c:4209)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source (pandas\\_libs\\parsers.c:8873)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: File b'companies.csv' does not exist"
     ]
    }
   ],
   "source": [
    "## Read file and then sort \n",
    "csv_file = \"a_companies.csv\"\n",
    "companies_df = pd.read_csv(csv_file, encoding=\"iso-8859-1\",\n",
    "                           parse_dates=[\"Founded Date\",\"Closed Date\",\n",
    "                                        \"Last Funding Date\"])\n",
    "companies_df = companies_df.sort_values(\"Company Name\").reset_index(drop=True)\n",
    "companies_df.head(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "companies_df = companies_df[[\"Company Name\", \"Founded Date\", \"Closed Date\", \n",
    "                             \"Number of Funding Rounds\", \"Last Funding Date\", \n",
    "                             \"Last Funding Amount\", \"Total Funding Amount\", \"Status\"]]\n",
    "companies_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "companies_pf = companies_df.rename(columns={\n",
    "    \"Company Name\" : \"Company\",\n",
    "    \"Number of Funding Rounds\" : \"Funding Rounds\"   \n",
    "})\n",
    "\n",
    "companies_pf.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Read file and then sort \n",
    "funding = \"a_funding_rounds.csv\"\n",
    "funding_df = pd.read_csv(funding, encoding=\"iso-8859-1\", \n",
    "                         parse_dates=[\"Announced On Date\"])\n",
    "funding_df.head(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "funding_df = funding_df[[\"Company Name\", \"Funding Type\", \"Money Raised\", \n",
    "                         \"Announced On Date\"]]\n",
    "funding_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "funding_pf = funding_df.rename(columns={\n",
    "    \"Company Name\" : \"Company\"\n",
    "})\n",
    "\n",
    "funding_pf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge companies_pf and funding_pf on Company\n",
    "merged_data = pd.merge(companies_pf, funding_pf,how=\"inner\",on=\"Company\")\n",
    "merged_data.head(5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Save Data to csv without index, with header\n",
    "#merged_data.to_csv(\"merged_company_funding.csv\", index=False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Read file and then sort \n",
    "org = \"organizations.csv\"\n",
    "org_df = pd.read_csv(org, encoding=\"iso-8859-1\")\n",
    "org_df.head(5)                              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename columns for readability\n",
    "new_file = org_df.rename(columns={\n",
    "    \"crunchbase_uuid\": \"UUID\",\n",
    "    \"Company Name\" : \"Company\",\n",
    "    \"homepage_url\" : \"Homepage\"   \n",
    "})\n",
    "\n",
    "new_file.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_file = new_file[[\"UUID\", \"Company\", \"Homepage\"]]\n",
    "new_file.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Save Data to csv without index, with header\n",
    "#new_file.to_csv(\"new_org.csv\", index=False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = pd.merge(merged_data, new_file,how=\"inner\",on=\"Company\")\n",
    "merged_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert fields to usable formats\n",
    "merged_df[\"Money Raised\"] = merged_df[\"Money Raised\"].str.strip() \\\n",
    "                                                   .replace('[\\$,]','', regex=True ).astype(np.int64)\n",
    "    \n",
    "merged_df[\"Last Funding Amount\"] = merged_df[\"Last Funding Amount\"] \\\n",
    "                                                   .replace('[\\$,]','', regex=True ).astype(np.int64)\n",
    "merged_df[\"Total Funding Amount\"] = merged_df[\"Total Funding Amount\"] \\\n",
    "                                                   .replace('[\\$,]','', regex=True ).astype(np.int64)\n",
    "merged_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# works\n",
    "for i, el in enumerate(merged_df.iterrows()):\n",
    "    val = el[1]['UUID'].replace(\"-\", \"\")\n",
    "    merged_df.set_value(i, \"UUID\", val)\n",
    "merged_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Save Data to csv without index, with header\n",
    "#merged_df.to_csv(\"all_files_merged_df.csv\", index=False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Expore the data\n",
    "min_found_date = min(merged_df[\"Founded Date\"])\n",
    "print(\"Min founded date \", min_found_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Expore the data\n",
    "max_close_date = max(merged_df[\"Closed Date\"])\n",
    "print(\"Max closed date \", max_close_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Expore the data\n",
    "# 286 records with Status == \"Closed\"\n",
    "closed_df = merged_df.loc[merged_df[\"Status\"] == \"Closed\"]\n",
    "closed_df.head(5)\n",
    "#print(len(closed_df.index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9302 records with Status == \"Operating\"\n",
    "oper_df = merged_df.loc[merged_df[\"Status\"] == \"Operating\"]\n",
    "oper_df.head(5)\n",
    "#print(len(oper_df.index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funding Type counts\n",
    "funding_type = merged_df[\"Funding Type\"].value_counts()\n",
    "funding_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# groupby Funding Type and count states within each type\n",
    "status = merged_df.groupby('Funding Type')[\"Status\"].value_counts()\n",
    "status.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.DataFrame(status).rename(columns={\n",
    "    'Status':'Count'})\n",
    "\n",
    "df1.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "series_d = merged_df[merged_df['Funding Type'] == \"Series D\"]\n",
    "series_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "funding_df = merged_df[merged_df['Funding Type'] == \"Seed\"]\n",
    "funding_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = merged_df[merged_df['Status'] == \"Was Acquired\"].count\n",
    "seed_df = merged_df[merged_df['Funding Type'] == \"Seed\"]\n",
    "\n",
    "seed_df = seed_df[seed_df['Status'] == \"Was Acquired\"]\n",
    "\n",
    "seed_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # Build a scatter plot for each data type\n",
    "# plt.scatter(seed_df[\"Funding Type\"], \n",
    "#             seed_df[\"Status\"],\n",
    "#             s=100, edgecolor=\"black\", linewidths=1, c=\"red\", marker=\"o\", \n",
    "#             alpha=0.8)\n",
    "\n",
    "# # Incorporate the other graph properties\n",
    "# plt.title(\"Funding Type Status\")\n",
    "# plt.ylabel(\"Funding Type\")\n",
    "# plt.xlabel(\"Status\")\n",
    "# plt.grid(True)\n",
    "\n",
    "# # Save the figure\n",
    "# plt.savefig(\"funding_type_scatter_chart.png\")\n",
    "\n",
    "# # Show plot\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# #seed = merged_df[merged_df['Status'] == \"Operating\"]\n",
    "# seed = merged_df[merged_df['Status'] == \"Was Acquired\"]\n",
    "# seed = merged_df[merged_df['Status'] == \"Closed\"]\n",
    "# seed = merged_df[merged_df['Status'] == \"IPO\"]\n",
    "\n",
    "# # # x-axis\n",
    "# seed_operating = seed.groupby([\"Status\"]).count()[\"Closed\"]\n",
    "# # #y-axis\n",
    "# seed_acquired = seed.groupby([\"Status\"]).count()[\"Was Acquired\"]\n",
    "\n",
    "# # # x-axis\n",
    "# seed_closedt = seed.groupby([\"Status\"]).count()[\"Closed\"]\n",
    "# # #y-axis\n",
    "# seed_ips = seed.groupby([\"Status\"]).count()[\"IPO\"]\n",
    "\n",
    "# # # # x-axis\n",
    "# # rural_ride_count = rural_cities.groupby([\"city\"]).count()[\"fare\"]\n",
    "# # # #y-axis\n",
    "# # rural_driver_count = rural_cities.groupby([\"city\"]).mean()[\"driver_count\"]\n",
    "\n",
    "# # # # x-axis\n",
    "# # suburban_ride_count = suburban_cities.groupby([\"city\"]).count()[\"fare\"]\n",
    "# # # #y-axis\n",
    "# # suburban_driver_count = suburban_cities.groupby([\"city\"]).mean()[\"driver_count\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "funding_sum = merged_df.groupby('Funding Type')[\"Money Raised\"].sum()\n",
    "funding_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['font.size'] = 12.0\n",
    "fig = plt.figure(figsize=[8, 8])\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "types_area = [\"Seed\", \"Series A\", \"Series B\", \"Series C\",\"Series D\" ]\n",
    "numbers = funding_sum\n",
    "colors = [\"gold\", \"lightcoral\", \"cyan\", \"lightgreen\", \"lightskyblue\"]\n",
    "explode = (0, 0.06, 0, 0, 0)\n",
    "\n",
    "x_axis = np.arange(0, len(types_area))\n",
    "\n",
    "ax.set_title(\"Funding Type Total Funding Percentages for Startups\", \n",
    "             weight='bold').set_fontsize('18')\n",
    "ax.pie(numbers, explode=explode, labels=types_area, colors=colors,\n",
    "       autopct=\"%1.2f%%\", textprops={'weight': 'bold', 'fontsize':'14'},\n",
    "       shadow=True, startangle=30)\n",
    "ax.axis(\"equal\")\n",
    "\n",
    "# Save the figure\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"funding_type_sum_pie_chart.png\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sum of counts is 10278\n",
    "funding_pct = merged_df.groupby('Funding Type')[\"Money Raised\"].count()\n",
    "funding_pct\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['font.size'] = 12.0\n",
    "\n",
    "fig = plt.figure(figsize=[8, 8])\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "types_area = [\"Seed\", \"Series A\", \"Series B\", \"Series C\",\"Series D\" ]\n",
    "numbers = funding_pct\n",
    "colors = [\"gold\", \"lightcoral\", \"cyan\", \"lightgreen\", \"lightskyblue\"]\n",
    "explode = (0.06, 0, 0, 0, 0)\n",
    "\n",
    "x_axis = np.arange(0, len(types_area))\n",
    "\n",
    "ax.set_title(\"Funding Type Percentages for Startups\", weight='bold').set_fontsize('18')\n",
    "\n",
    "ax.pie(numbers, explode=explode, \n",
    "       labels=types_area, colors=colors,\n",
    "       autopct=\"%1.2f%%\", textprops={'weight': 'bold', 'fontsize':'14'}, \n",
    "       shadow=True, startangle=30, pctdistance=.85, labeldistance=1.1)\n",
    "\n",
    "ax.axis(\"equal\")\n",
    "\n",
    "# Save the figure\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"funding_type_count_pie_chart.png\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# y axis for funding_average_per_type_bar_chart \n",
    "fund_avgs = np.array(merged_df.groupby('Funding Type')[\"Money Raised\"].median())\n",
    "#fund_avgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# x axis for funding_average_per_type_bar_chart \n",
    "funding_types = merged_df[\"Funding Type\"].unique()\n",
    "#funding_types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# funding_average_per_type_bar_chart \n",
    "# Dependencies\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "x_axis = np.arange(1, len(funding_types)+ 1)\n",
    "xtick_locations = [x for x in x_axis]\n",
    "\n",
    "ax.set_title(\"Funding Type Average Investment\", weight='bold').set_fontsize('18')\n",
    "ax.set_xlabel(\"Funding Types\", weight='bold').set_fontsize('14')\n",
    "ax.set_ylabel(\"Average Investments(10MM)\", weight='bold').set_fontsize('14')\n",
    "\n",
    "ax.set_xlim(0, len(funding_types)+ 1)\n",
    "\n",
    "plt.ticklabel_format(style = 'plain')\n",
    "\n",
    "ax.bar(x_axis, fund_avgs, facecolor=\"red\", width=.4)\n",
    "ax.set_xticks(xtick_locations)\n",
    "\n",
    "ax.set_xticklabels(funding_types, rotation=35, weight='bold')\n",
    "ax.set_yticklabels([0,10,20,30,40,50,60,70],\n",
    "                   rotation=360, weight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"funding_type_avg_investment_bar_chart.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## File without urls required for screen scraping.  \n",
    "## Read file and then sort \n",
    "csv_file = \"top_50_orgs.csv\"\n",
    "top_50_orgs = pd.read_csv(csv_file, encoding=\"iso-8859-1\", parse_dates=[\"Announced On Date\"])\n",
    "top_50_orgs = top_50_orgs.sort_values(\"Funding Type\").reset_index(drop=True)\n",
    "top_50_orgs[\"Homepage\"] = top_50_orgs[\"Homepage\"].replace(np.nan, '', regex=True)\n",
    "top_50_orgs.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # The original top_orgs.xslx had the wrong url so this is a \n",
    "# # manual process to get the Homepage for Screen Scraping.\n",
    "# # To sutomate the url retrievel process you can build off \n",
    "# # this manual process.\n",
    "# # \n",
    "# # 1) Remove duplicates from copy of merged_df\n",
    "# # 2) Copy the output into top_orgs_search_results.txt\n",
    "# # 3) Manually copy the homepage url into top_orgs.xlsx and\n",
    "# #    create top_orgs.csv.\n",
    "# # 4) Searched organizations.xlsx manually and found Ripcord, \n",
    "# #    IndoorAtlas, and Truss.\n",
    "# # 5) Found 46 of 50 urls.\n",
    "\n",
    "# Copy merged_df to a temp data frame for processing\n",
    "tmp_merged_df = merged_df\n",
    "\n",
    "# Remove duplicates method 1:\n",
    "# tmp_merged_df = tmp_merged_df.groupby([\"Company\",\"Homepage\"]).max()\n",
    "# tmp_merged_df.reset_index(inplace=True)\n",
    "# or\n",
    "# Remove duplicates method 2:\n",
    "tmp_merged_df = tmp_merged_df.drop_duplicates(subset=[\"Company\",\"Homepage\"],\n",
    "                                              keep=\"last\").reset_index(drop=True)\n",
    "\n",
    "count = 0\n",
    "\n",
    "## Iterate over the top_orgs rows and lookup the homepage via the Company\n",
    "## This doesn't give what I want so I manually copied the url into top_orgs.xlsx\n",
    "## and created top_orgs.csv.\n",
    "for index, row in top_50_orgs.iterrows():\n",
    "    try:\n",
    "        # Use Company to get Homepage\n",
    "        print(\"This is the result for: \", row[\"Company\"])\n",
    "        #print(tmp_merged_df.loc[tmp_merged_df[\"Company\"] == row[\"Company\"],[\"Homepage\"]].values)\n",
    "        print(tmp_merged_df.loc[tmp_merged_df[\"Company\"] == row[\"Company\"],[\"Homepage\"]])\n",
    "        count += 1\n",
    "        ## Set the cell info for Homepage (Doesn't work!)\n",
    "        # example: print(df.loc[df['D'] == 14]['A'].values)\n",
    "        #url = tmp_merged_df.loc[tmp_merged_df[\"Company\"] == row[\"Company\"],[\"Homepage\"]]\n",
    "        #top_50_orgs.set_value(index, \"Homepage\", url)\n",
    "    except:\n",
    "        print(\"Error for: \", row[\"Company\"])  \n",
    "\n",
    "print(\"top_orgs rows processed: \", count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## File with url manually added for screen scraping using\n",
    "## the output file top_orgs_match_orgs_results.docx\n",
    "## Read file and then sort \n",
    "csv_file = \"top_orgs.csv\"\n",
    "top_orgs = pd.read_csv(csv_file, encoding=\"iso-8859-1\", parse_dates=[\"Announced On Date\"])\n",
    "top_orgs = top_orgs.sort_values(\"Funding Type\").reset_index(drop=True)\n",
    "top_orgs.head(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "top_orgs[\"Money Raised\"] = top_orgs[\"Money Raised\"].replace('[\\$,]','', regex=True).astype(np.int64)\n",
    "top_orgs[\"Homepage\"] = top_orgs[\"Homepage\"].replace(np.nan, '', regex=True)\n",
    "\n",
    "top_orgs[\"Total Visits\"] = \"\"\n",
    "top_orgs[\"Avg Visit Duration\"] = \"\"\n",
    "top_orgs[\"Pages Per Visit\"] = \"\"\n",
    "top_orgs[\"Bounce Rate\"] = \"\"\n",
    "print(len(top_orgs.index))\n",
    "top_orgs.head(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Save file with urls prior to scraping.to csv without index, with header\n",
    "#top_orgs.to_csv(\"pre-screen_scrape_data.csv\", index=False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Web scraping program with 20 second sleep time.\n",
    "## Consider using a smaller subset of data for scraping.\n",
    "\n",
    "##  Test URL https://www.similarweb.com/website/shotput.com#overview \n",
    "\n",
    "## Splinter documenation:\n",
    "## https://github.com/douglasmiranda/splinter-examples/blob/master/another_examples/screenshot.py\n",
    "## http://splinter.readthedocs.io/en/latest/tutorial.html\n",
    "\n",
    "## BeautifulSoup Documentation:\n",
    "## https://www.crummy.com/software/BeautifulSoup/bs4/doc\n",
    "\n",
    "\n",
    "# from splinter import Browser\n",
    "# import csv\n",
    "# from bs4 import BeautifulSoup \n",
    "# import time\n",
    "\n",
    "# # browser = Browser('chrome')\n",
    "# # browser = Browser('firefox')\n",
    "\n",
    "# with Browser() as browser:\n",
    "#     # Visit URL\n",
    "#     url1 = \"https://www.similarweb.com/website/\" \n",
    "#     url_suffix = \"#overview\"\n",
    "    \n",
    "#     # loop thru the urls top_orgs data frame \n",
    "#     counter = 0\n",
    "#     for index, row in top_orgs.iterrows():\n",
    "#         url = \"\"\n",
    "#         #print(type(row[\"Homepage\"]))\n",
    "#         url2 = row[\"Homepage\"].split('/')\n",
    "#         if len(url2) > 2:\n",
    "#             counter += 1\n",
    "#             url = url1 + url2[2] + url_suffix\n",
    "#             print(\"url counter: \", counter,end=\" - \")\n",
    "#             print(url)\n",
    "        \n",
    "#             try:\n",
    "#                 browser.visit(url)\n",
    "#                 time.sleep(20)\n",
    "#                 html = browser.html\n",
    "#                 soup = BeautifulSoup(html, \"html.parser\")\n",
    "            \n",
    "#                 #print(soup.prettify())\n",
    "   \n",
    "#                 count = 0\n",
    "#                 # Grab the 4 activity stats from the html \n",
    "#                 for line in soup.findAll('span',class_=\"engagementInfo-valueNumber js-countValue\"):\n",
    "#                     print(line.get_text())\n",
    "#                     count += 1\n",
    "#                     # Write to a data frame\n",
    "#                     if count == 1:\n",
    "#                         top_orgs.set_value(index, \"Total Visits\", line.getText())\n",
    "#                     elif count == 2:\n",
    "#                         top_orgs.set_value(index, \"Avg Visit Duration\", line.getText())\n",
    "#                     elif count == 3:\n",
    "#                         top_orgs.set_value(index, \"Pages Per Visit\", line.getText())\n",
    "#                     else:\n",
    "#                         top_orgs.set_value(index, \"Bounce Rate\", line.getText())\n",
    "#             except:\n",
    "#                 print(\"Error with url: \", url)  \n",
    "# top_orgs\n",
    "# print(\"urls processed: \", counter)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# If we screen scraped then save data to csv without index, with header\n",
    "#top_orgs.to_csv(\"screen_scrape_data.csv\", index=False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Reload data to work with Screen scrape data without more scraping\n",
    "## Read file \n",
    "\n",
    "csv_file = \"screen_scrape_data.csv\"\n",
    "top_orgs = pd.read_csv(csv_file, encoding=\"iso-8859-1\", parse_dates=[\"Announced On Date\"])\n",
    "\n",
    "top_orgs[\"Money Raised\"] = top_orgs[\"Money Raised\"].replace('[\\$,]','', regex=True ).astype(np.int64)\n",
    "top_orgs[\"Homepage\"] = top_orgs[\"Homepage\"].replace(np.nan, '', regex=True)\n",
    "top_orgs[\"Total Visits\"] = top_orgs[\"Total Visits\"].replace(np.nan, 0, regex=True)\n",
    "#top_orgs[\"Avg Visit Duration\"] = top_orgs[\"Avg Visit Duration\"].replace(np.nan, '', regex=True)\n",
    "top_orgs[\"Pages Per Visit\"] = top_orgs[\"Pages Per Visit\"].replace(np.nan, 0, regex=True)\n",
    "top_orgs[\"Bounce Rate\"] = top_orgs[\"Bounce Rate\"].replace(np.nan, 0, regex=True)\n",
    "\n",
    "top_orgs.head(7)\n",
    "#top_orgs.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # Remove rows we don't need to process\n",
    "# top_orgs = top_orgs.loc[top_orgs[\"Total Visits\"]!= \"\",[\"Company\",\"Funding Type\",\n",
    "#                            \"Money Raised\",\"Announced On Date\",\"Homepage\",\"Total Visits\",\n",
    "#                            \"Avg Visit Duration\",\"Pages Per Visit\",\"Bounce Rate\"]]\n",
    "\n",
    "#top_orgs.dtypes\n",
    "#print(len(top_orgs.index))\n",
    "#top_orgs.head(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## This code doesn't work yet...  39 rows with 12K like strings\n",
    "##                                 5 rows with 5.36M like strings\n",
    "\n",
    "## Total Visits: strip non-numeric characters and convert to float64\n",
    "for index, row in top_orgs.iterrows():\n",
    "    print(\"index is: \", index)\n",
    "    print(row[\"Total Visits\"])\n",
    "    print(type(row[\"Total Visits\"]))\n",
    "    \n",
    "    string1 = str(row[\"Total Visits\"])\n",
    "    if \"K\" in string1:\n",
    "        string1 = string1.replace('K','')\n",
    "        string1 = string1.replace('<','')\n",
    "        flstring1 = float(string1) * 1000\n",
    "        print(\"string1 with a K is \", string1)\n",
    "        top_orgs.set_value(index, \"Total Visits\", flstring1)\n",
    "    else:\n",
    "        string1 = string1.replace('M','')\n",
    "        string1 = string1.replace('<','')\n",
    "        flstring1 = float(string1) * 1000000\n",
    "        print(\"other string1 is \", string1)\n",
    "        top_orgs.set_value(index, \"Total Visits\", flstring1)\n",
    "    \n",
    "    #row[\"Total Visits\"] = row[\"Total Visits\"].replace('[<>KM]','',regex=True).astype(np.float64)* 1000\n",
    "\n",
    "top_orgs[\"Total Visits\"] = top_orgs[\"Total Visits\"].astype(np.float64)\n",
    "pd.set_option('display.float_format', lambda x: '%.2f' % x)\n",
    "    \n",
    "top_orgs   \n",
    "#top_orgs.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Convert string columns into usable data types\n",
    "\n",
    "## Convert a column to datetime64 type with only the time?\n",
    "## pd.to_datetime.dt.time is a datetime.time object but not datetime64.\n",
    "## Therefore, it is correct that the dtype for the column is object.  \n",
    "## Converting Avg Visit Duration to datetime.time renders the desired \n",
    "## format 00:00:00.\n",
    "## Check out this documentation on Time series:\n",
    "## https://pandas.pydata.org/pandas-docs/stable/timeseries.html\n",
    "\n",
    "top_orgs[\"Avg Visit Duration\"] = pd.to_datetime(top_orgs[\"Avg Visit Duration\"],format=\"%H:%M:%S\").dt.time\n",
    "\n",
    "top_orgs[\"Bounce Rate\"] = top_orgs[\"Bounce Rate\"].replace('[%]','', regex=True ).astype(np.float64)\n",
    "\n",
    "top_orgs[\"Pages Per Visit\"] = top_orgs[\"Pages Per Visit\"].astype(np.float64)\n",
    "\n",
    "#top_orgs.dtypes\n",
    "top_orgs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
